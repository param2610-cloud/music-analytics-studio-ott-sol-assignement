{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "195a99ef",
   "metadata": {},
   "source": [
    "# Notebook 02 â€” Cleaning, Normalization & Combine\n",
    "\n",
    "**Goal**: Unify column names, parse dates, make numeric, combine into `df_all`.\n",
    "\n",
    "**Important**: Airtel and Wynk are the SAME platform. Airtel data will be merged into Wynk by matching on track/artist/album.\n",
    "\n",
    "\n",
    "**Canonical Columns**:`['source', 'activity_period', 'year_month', 'store_name', 'country', 'artist', 'album', 'track', 'revenue', 'stream_count', 'unit_type', 'project_code']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af18c105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "DATA_DIR = Path('../data')\n",
    "OUTPUT_DIR = Path('../outputs/cleaned')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define Canonical Columns\n",
    "CANONICAL_COLS = [\n",
    "    'source', 'activity_period', 'year_month', 'store_name', 'country', \n",
    "    'artist', 'album', 'track', 'revenue', 'stream_count', 'unit_type', 'project_code'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36f99a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw files loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load Raw Data\n",
    "try:\n",
    "    airtel_raw = pd.read_csv(DATA_DIR/'airtel-report.csv')\n",
    "    jio_raw = pd.read_csv(DATA_DIR/'jiosaavn-report.csv')\n",
    "    wynk_raw = pd.read_csv(DATA_DIR/'wynk-report.csv')\n",
    "    print(\"Raw files loaded.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f291a8",
   "metadata": {},
   "source": [
    "## Column Mapping Definitions\n",
    "Define how each raw file maps to the canonical schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a645f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mappings based on EDA (Notebook 01)\n",
    "# Key = Canonical Name, Value = Raw Column Name\n",
    "\n",
    "AIRTEL_MAP = {\n",
    "    'track': 'Song Name',\n",
    "    'album': 'Album Name',\n",
    "    'artist': 'Artist',\n",
    "    'revenue': 'Rev'\n",
    "    # Missing: stream_count, activity_period, country, isrc/project_code\n",
    "}\n",
    "\n",
    "JIO_MAP = {\n",
    "    'track': 'song_name',\n",
    "    'album': 'album_name',\n",
    "    'artist': 'artist_name',\n",
    "    'revenue': 'income',\n",
    "    'stream_count': 'total',\n",
    "    'project_code': 'isrc'\n",
    "}\n",
    "\n",
    "WYNK_MAP = {\n",
    "    'track': 'song_name',\n",
    "    'album': 'album_name',\n",
    "    'artist': 'artist',\n",
    "    'revenue': 'income',\n",
    "    'stream_count': 'total',\n",
    "    'project_code': 'isrc'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85ea19d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_normalize(df, source_label, col_map, default_country='India'):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Rename columns based on map\n",
    "    # Invert map for renaming: {Raw: Canonical}\n",
    "    rename_map = {v: k for k, v in col_map.items()}\n",
    "    df = df.rename(columns=rename_map)\n",
    "    \n",
    "    # 2. Add missing canonical columns with default values (NaN or specific)\n",
    "    for col in CANONICAL_COLS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "            \n",
    "    # 3. Set Source and Store Name (default to source_label if store_name missing)\n",
    "    df['source'] = source_label\n",
    "    if pd.isna(df['store_name']).all():\n",
    "        df['store_name'] = source_label.capitalize()\n",
    "\n",
    "    # 4. Clean Revenue\n",
    "    # Remove currency symbols if present, then convert\n",
    "    if df['revenue'].dtype == 'object':\n",
    "        df['revenue'] = df['revenue'].astype(str).str.replace(r'[^\\d.-]', '', regex=True)\n",
    "    df['revenue'] = pd.to_numeric(df['revenue'], errors='coerce').fillna(0)\n",
    "    \n",
    "    # 5. Clean Stream Count\n",
    "    if 'stream_count' in df.columns:\n",
    "        df['stream_count'] = pd.to_numeric(df['stream_count'], errors='coerce').fillna(0)\n",
    "    \n",
    "    # 6. Date / Activity Period Parsing\n",
    "    # TODO: If date is missing in CSV, we might need to infer from filename or user input.\n",
    "    # For now, we look for common date columns if not already mapped.\n",
    "    date_cols = [c for c in df.columns if 'date' in c.lower() or 'period' in c.lower()]\n",
    "    if pd.isna(df['activity_period']).all() and date_cols:\n",
    "        # Try the first found date column\n",
    "        print(f\"[{source_label}] Inferring date from column: {date_cols[0]}\")\n",
    "        df['activity_period'] = pd.to_datetime(df[date_cols[0]], errors='coerce')\n",
    "    else:\n",
    "        # Ensure it's datetime even if empty\n",
    "        df['activity_period'] = pd.to_datetime(df['activity_period'], errors='coerce')\n",
    "        \n",
    "    # Create year_month\n",
    "    df['year_month'] = df['activity_period'].dt.to_period('M').astype(str).replace('NaT', np.nan)\n",
    "\n",
    "    # 7. Country Defaults\n",
    "    if 'country' in df.columns:\n",
    "        df['country'] = df['country'].fillna(default_country)\n",
    "        \n",
    "    # 8. String Normalization (Artist, Album, Track)\n",
    "    for col in ['artist', 'album', 'track']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip().str.title()\n",
    "\n",
    "    # Return only canonical columns to ensure schema match\n",
    "    return df[CANONICAL_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2df74079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Airtel (will merge into Wynk)...\n",
      "[wynk] Inferring date from column: activity_period\n",
      "Cleaning JioSaavn...\n",
      "[jiosaavn] Inferring date from column: activity_period\n",
      "Cleaning Wynk...\n",
      "[wynk] Inferring date from column: activity_period\n",
      "Airtel (before merge): (4, 12)\n",
      "Jio: (2666, 12)\n",
      "Wynk (before merge): (6212, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_53731/332455443.py:42: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['year_month'] = df['activity_period'].dt.to_period('M').astype(str).replace('NaT', np.nan)\n",
      "/tmp/ipykernel_53731/332455443.py:42: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['year_month'] = df['activity_period'].dt.to_period('M').astype(str).replace('NaT', np.nan)\n",
      "/tmp/ipykernel_53731/332455443.py:42: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['year_month'] = df['activity_period'].dt.to_period('M').astype(str).replace('NaT', np.nan)\n"
     ]
    }
   ],
   "source": [
    "# Apply Cleaning\n",
    "print(\"Cleaning Airtel (will merge into Wynk)...\")\n",
    "airtel_clean = clean_and_normalize(airtel_raw, 'wynk', AIRTEL_MAP)  # Note: source='wynk'\n",
    "\n",
    "print(\"Cleaning JioSaavn...\")\n",
    "jio_clean = clean_and_normalize(jio_raw, 'jiosaavn', JIO_MAP)\n",
    "\n",
    "print(\"Cleaning Wynk...\")\n",
    "wynk_clean = clean_and_normalize(wynk_raw, 'wynk', WYNK_MAP)\n",
    "\n",
    "# Check shapes before merge\n",
    "print(f\"Airtel (before merge): {airtel_clean.shape}\")\n",
    "print(f\"Jio: {jio_clean.shape}\")\n",
    "print(f\"Wynk (before merge): {wynk_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ed5b588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MERGING AIRTEL INTO WYNK (SAME PLATFORM)\n",
      "============================================================\n",
      "Found 0 matching tracks between Airtel and Wynk\n",
      "Airtel - Matched: 0, Unmatched: 4\n",
      "âœ… Added 4 new Airtel records to Wynk\n",
      "\n",
      "Wynk (after merge): (6216, 12)\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š FINAL Combined Shape: (8882, 12)\n",
      "   - JioSaavn: 2,666 rows\n",
      "   - Wynk (incl. Airtel): 6,216 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>activity_period</th>\n",
       "      <th>year_month</th>\n",
       "      <th>store_name</th>\n",
       "      <th>country</th>\n",
       "      <th>artist</th>\n",
       "      <th>album</th>\n",
       "      <th>track</th>\n",
       "      <th>revenue</th>\n",
       "      <th>stream_count</th>\n",
       "      <th>unit_type</th>\n",
       "      <th>project_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jiosaavn</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jiosaavn</td>\n",
       "      <td>India</td>\n",
       "      <td>Jojo  Mrinal Mukherjee  Prem Kumar</td>\n",
       "      <td>Tara Maa Ki Mahima Aapar</td>\n",
       "      <td>Tara Maiya Tere Darbar</td>\n",
       "      <td>0.675</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INT441651504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jiosaavn</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jiosaavn</td>\n",
       "      <td>India</td>\n",
       "      <td>Sanchita  Shaan  Swarna</td>\n",
       "      <td>Hiya Deba Kaak</td>\n",
       "      <td>Lovely Lovely Night</td>\n",
       "      <td>0.375</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INT441629102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jiosaavn</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jiosaavn</td>\n",
       "      <td>India</td>\n",
       "      <td>Indradeep Dasgupta</td>\n",
       "      <td>Kanamachi</td>\n",
       "      <td>Mon Bawra</td>\n",
       "      <td>36.150</td>\n",
       "      <td>482.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INT441632205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jiosaavn</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jiosaavn</td>\n",
       "      <td>India</td>\n",
       "      <td>Raja Narayan Deb</td>\n",
       "      <td>Monchuri</td>\n",
       "      <td>Bolo Bolo Bolo Shobe</td>\n",
       "      <td>0.450</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INT441636701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jiosaavn</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jiosaavn</td>\n",
       "      <td>India</td>\n",
       "      <td>Jojo  Mrinal Mukherjee  Prem Kumar</td>\n",
       "      <td>Tara Maa Ki Mahima Aapar</td>\n",
       "      <td>Natraj Nache Re</td>\n",
       "      <td>0.675</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INT441651502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     source activity_period  year_month store_name country  \\\n",
       "0  jiosaavn             NaT         NaN   Jiosaavn   India   \n",
       "1  jiosaavn             NaT         NaN   Jiosaavn   India   \n",
       "2  jiosaavn             NaT         NaN   Jiosaavn   India   \n",
       "3  jiosaavn             NaT         NaN   Jiosaavn   India   \n",
       "4  jiosaavn             NaT         NaN   Jiosaavn   India   \n",
       "\n",
       "                               artist                     album  \\\n",
       "0  Jojo  Mrinal Mukherjee  Prem Kumar  Tara Maa Ki Mahima Aapar   \n",
       "1             Sanchita  Shaan  Swarna            Hiya Deba Kaak   \n",
       "2                  Indradeep Dasgupta                 Kanamachi   \n",
       "3                    Raja Narayan Deb                  Monchuri   \n",
       "4  Jojo  Mrinal Mukherjee  Prem Kumar  Tara Maa Ki Mahima Aapar   \n",
       "\n",
       "                    track  revenue  stream_count  unit_type  project_code  \n",
       "0  Tara Maiya Tere Darbar    0.675           9.0        NaN  INT441651504  \n",
       "1     Lovely Lovely Night    0.375           5.0        NaN  INT441629102  \n",
       "2               Mon Bawra   36.150         482.0        NaN  INT441632205  \n",
       "3    Bolo Bolo Bolo Shobe    0.450           6.0        NaN  INT441636701  \n",
       "4         Natraj Nache Re    0.675           9.0        NaN  INT441651502  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Merge Airtel into Wynk (same platform)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MERGING AIRTEL INTO WYNK (SAME PLATFORM)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create normalized keys for matching\n",
    "airtel_clean['match_key'] = (airtel_clean['track'].str.lower().str.strip() + '|' + \n",
    "                              airtel_clean['artist'].str.lower().str.strip() + '|' + \n",
    "                              airtel_clean['album'].str.lower().str.strip())\n",
    "\n",
    "wynk_clean['match_key'] = (wynk_clean['track'].str.lower().str.strip() + '|' + \n",
    "                            wynk_clean['artist'].str.lower().str.strip() + '|' + \n",
    "                            wynk_clean['album'].str.lower().str.strip())\n",
    "\n",
    "# Find matches\n",
    "matched_keys = set(airtel_clean['match_key']) & set(wynk_clean['match_key'])\n",
    "print(f\"Found {len(matched_keys)} matching tracks between Airtel and Wynk\")\n",
    "\n",
    "# Split Airtel into matched and unmatched\n",
    "airtel_matched = airtel_clean[airtel_clean['match_key'].isin(matched_keys)].copy()\n",
    "airtel_unmatched = airtel_clean[~airtel_clean['match_key'].isin(matched_keys)].copy()\n",
    "\n",
    "print(f\"Airtel - Matched: {len(airtel_matched)}, Unmatched: {len(airtel_unmatched)}\")\n",
    "\n",
    "# For matched records, aggregate revenue and streams\n",
    "if len(airtel_matched) > 0:\n",
    "    # Aggregate Airtel matched data by match_key\n",
    "    airtel_agg = airtel_matched.groupby('match_key', as_index=False).agg({\n",
    "        'revenue': 'sum',\n",
    "        'stream_count': 'sum'\n",
    "    })\n",
    "    \n",
    "    # Merge aggregated Airtel data into Wynk\n",
    "    wynk_clean = wynk_clean.merge(airtel_agg, on='match_key', how='left', suffixes=('', '_airtel'))\n",
    "    \n",
    "    # Add Airtel revenue/streams to Wynk where matched\n",
    "    wynk_clean['revenue'] = wynk_clean['revenue'] + wynk_clean['revenue_airtel'].fillna(0)\n",
    "    wynk_clean['stream_count'] = wynk_clean['stream_count'] + wynk_clean['stream_count_airtel'].fillna(0)\n",
    "    \n",
    "    # Drop temporary columns\n",
    "    wynk_clean = wynk_clean.drop(columns=['revenue_airtel', 'stream_count_airtel'])\n",
    "    \n",
    "    print(f\"âœ… Merged {len(airtel_matched)} Airtel records into existing Wynk records\")\n",
    "\n",
    "# Add unmatched Airtel records as new entries\n",
    "if len(airtel_unmatched) > 0:\n",
    "    airtel_unmatched = airtel_unmatched.drop(columns=['match_key'])\n",
    "    wynk_clean = pd.concat([wynk_clean, airtel_unmatched], ignore_index=True)\n",
    "    print(f\"âœ… Added {len(airtel_unmatched)} new Airtel records to Wynk\")\n",
    "\n",
    "# Drop match_key column\n",
    "wynk_clean = wynk_clean.drop(columns=['match_key'])\n",
    "\n",
    "print(f\"\\nWynk (after merge): {wynk_clean.shape}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Combine final dataframes (JioSaavn + Wynk)\n",
    "df_all = pd.concat([jio_clean, wynk_clean], ignore_index=True, sort=False)\n",
    "\n",
    "print(f\"\\nðŸ“Š FINAL Combined Shape: {df_all.shape}\")\n",
    "print(f\"   - JioSaavn: {len(jio_clean):,} rows\")\n",
    "print(f\"   - Wynk (incl. Airtel): {len(wynk_clean):,} rows\")\n",
    "display(df_all.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e87bca4",
   "metadata": {},
   "source": [
    "## Post-Combine Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fa77f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š FINAL PLATFORM DISTRIBUTION:\n",
      "                 revenue  stream_count  row_count\n",
      "store_name                                       \n",
      "JioSaavn     6940.200000       92536.0       2666\n",
      "Wynk Music  22760.579356      404885.0       6216\n"
     ]
    }
   ],
   "source": [
    "# 1. Normalize Artist Names (Simple Strip/Lower for grouping, Title for display)\n",
    "df_all['artist'] = df_all['artist'].fillna('Unknown Artist')\n",
    "\n",
    "# 2. Normalize Store Names (Airtel is already merged into Wynk)\n",
    "store_map = {\n",
    "    'Jiosaavn': 'JioSaavn',\n",
    "    'Wynk': 'Wynk Music'\n",
    "}\n",
    "df_all['store_name'] = df_all['store_name'].replace(store_map)\n",
    "\n",
    "print(\"\\nðŸ“Š FINAL PLATFORM DISTRIBUTION:\")\n",
    "print(df_all.groupby('store_name').agg({\n",
    "    'revenue': 'sum',\n",
    "    'stream_count': 'sum',\n",
    "    'track': 'count'\n",
    "}).rename(columns={'track': 'row_count'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88135468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved canonical dataset to ../outputs/cleaned/df_all.parquet\n"
     ]
    }
   ],
   "source": [
    "# Save to Parquet\n",
    "output_path = OUTPUT_DIR / 'df_all.parquet'\n",
    "df_all.to_parquet(output_path, index=False)\n",
    "print(f\"Saved canonical dataset to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c2d172",
   "metadata": {},
   "source": [
    "## Cleaning Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58ca6241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log written to ../logs/cleaning.log\n"
     ]
    }
   ],
   "source": [
    "log_path = Path('../logs/cleaning.log')\n",
    "log_path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "with open(log_path, 'w') as f:\n",
    "    f.write(f\"Cleaning Log - {datetime.datetime.now()}\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\")\n",
    "    f.write(f\"Total Rows Processed: {len(df_all)}\\n\")\n",
    "    f.write(f\"Sources: {df_all['source'].unique()}\\n\")\n",
    "    f.write(f\"Missing Dates: {df_all['activity_period'].isna().sum()}\\n\")\n",
    "    f.write(f\"Total Revenue: {df_all['revenue'].sum():.2f}\\n\")\n",
    "    \n",
    "print(f\"Log written to {log_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
